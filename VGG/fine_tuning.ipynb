{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader_image_classification import ImageTransform, make_datapath_list, HymenopteraDataset\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/hymenoptera_data/train/**/*.jpg\n",
      "./data/hymenoptera_data/val/**/*.jpg\n"
     ]
    }
   ],
   "source": [
    "#get datapath list\n",
    "train_list = make_datapath_list(phase=\"train\")\n",
    "val_list = make_datapath_list(phase=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 224\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "train_dataset = HymenopteraDataset(\n",
    "    file_list=train_list, transform=ImageTransform(size, mean, std), phase=\"train\"\n",
    ")\n",
    "val_dataset = HymenopteraDataset(\n",
    "    file_list=val_list, transform=ImageTransform(size, mean, std), phase=\"val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset by batch\n",
    "batch_size = 32\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_dict = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"val\": val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "features.2.bias\n",
      "features.5.weight\n",
      "features.5.bias\n",
      "features.7.weight\n",
      "features.7.bias\n",
      "features.10.weight\n",
      "features.10.bias\n",
      "features.12.weight\n",
      "features.12.bias\n",
      "features.14.weight\n",
      "features.14.bias\n",
      "features.17.weight\n",
      "features.17.bias\n",
      "features.19.weight\n",
      "features.19.bias\n",
      "features.21.weight\n",
      "features.21.bias\n",
      "features.24.weight\n",
      "features.24.bias\n",
      "features.26.weight\n",
      "features.26.bias\n",
      "features.28.weight\n",
      "features.28.bias\n",
      "classifier.0.weight\n",
      "classifier.0.bias\n",
      "classifier.3.weight\n",
      "classifier.3.bias\n",
      "classifier.6.weight\n",
      "classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "use_pretrained = torchvision.models.VGG16_Weights.DEFAULT\n",
    "net = models.vgg16(weights=use_pretrained)\n",
    "\n",
    "net.classifier[6] = nn.Linear(in_features=4096, out_features=2)\n",
    "net.train()\n",
    "\n",
    "for name, _ in net.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight updated in param 1\n",
      "features.0.bias updated in param 1\n",
      "features.2.weight updated in param 1\n",
      "features.2.bias updated in param 1\n",
      "features.5.weight updated in param 1\n",
      "features.5.bias updated in param 1\n",
      "features.7.weight updated in param 1\n",
      "features.7.bias updated in param 1\n",
      "features.10.weight updated in param 1\n",
      "features.10.bias updated in param 1\n",
      "features.12.weight updated in param 1\n",
      "features.12.bias updated in param 1\n",
      "features.14.weight updated in param 1\n",
      "features.14.bias updated in param 1\n",
      "features.17.weight updated in param 1\n",
      "features.17.bias updated in param 1\n",
      "features.19.weight updated in param 1\n",
      "features.19.bias updated in param 1\n",
      "features.21.weight updated in param 1\n",
      "features.21.bias updated in param 1\n",
      "features.24.weight updated in param 1\n",
      "features.24.bias updated in param 1\n",
      "features.26.weight updated in param 1\n",
      "features.26.bias updated in param 1\n",
      "features.28.weight updated in param 1\n",
      "features.28.bias updated in param 1\n",
      "classifier.0.weight updated in param 2\n",
      "classifier.0.bias updated in param 2\n",
      "classifier.3.weight updated in param 2\n",
      "classifier.3.bias updated in param 2\n",
      "classifier.6.weight updated in param 3\n",
      "classifier.6.bias updated in param 3\n"
     ]
    }
   ],
   "source": [
    "#fine tuning the model\n",
    "params_to_update1, params_to_update2, params_to_update3 = [], [], []\n",
    "update_list1 = [\n",
    "    \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\"\n",
    "]\n",
    "update_list2 = [\n",
    "    \"classifier.6.weight\", \"classifier.6.bias\"\n",
    "]\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    #update feature layer\n",
    "    if \"features\" in name:\n",
    "        param.requires_grad = True\n",
    "        params_to_update1.append(param)\n",
    "        print(f\"{name} updated in param 1\")\n",
    "    elif name in update_list1:\n",
    "        param.requires_grad = True\n",
    "        params_to_update2.append(param)\n",
    "        print(f\"{name} updated in param 2\")\n",
    "    elif name in update_list2:\n",
    "        param.requires_grad = True\n",
    "        params_to_update3.append(param)\n",
    "        print(f\"{name} updated in param 3\")\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "        print(f\"{name} not update when train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([\n",
    "    {\"params\": params_to_update1, \"lr\": 1e-4},\n",
    "    {\"params\": params_to_update2, \"lr\": 5e-4},\n",
    "    {\"params\": params_to_update3, \"lr\": 1e-3},\n",
    "], momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Device:{device}\")\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    #automatically choose best convolution algorithm\n",
    "    #if input size == static, this will work\n",
    "    #else may slow down\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}\", end=\"\\n====================\\n\")\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "\n",
    "            for inputs, labels in tqdm(dataloaders_dict[phase], colour=\"green\"):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase==\"train\"):\n",
    "                    outputs = net(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = outputs.argmax(1)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            print(f\"{phase} Loss: {epoch_loss/len(dataloaders_dict[phase].dataset):.4f} Acc: {epoch_corrects.double()/len(dataloaders_dict[phase].dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:cpu\n",
      "Epoch: 1/5\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[32m          \u001b[0m| 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
